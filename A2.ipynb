{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the dataset for NLP with Disaster Tweets Kaggle competition\n",
    "!kaggle competitions download -c nlp-getting-started\n",
    "!unzip nlp-getting-started.zip -d nlp-getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "og_train_df = pd.read_csv(\"nlp-getting-started/train.csv\")\n",
    "og_test_df = pd.read_csv(\"nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_train_df[\"id\"].value_counts().sum()\n",
    "og_test_df[\"id\"].value_counts().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"target\"].value_counts().plot(kind='barh', color='blue', alpha=0.7)\n",
    "plt.pie(og_train_df[\"target\"].value_counts(), labels=[\"real\", \"not real\"])\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"Distribution of Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Preprocess the dataset before splitting it into the 70/30 distribution required by the assignment. Although we have followed the instruction's recommendations. We took some liberties with the order and depth to which we followed the instructions. Thus, our preprocesing consists of:\n",
    "\n",
    "1. Removing punctuation  - This is helpful in standardizing the texts, but can create some isues in multiword lexical units and names. To solve this, a name entity recognition should be applied before this step.\n",
    "\n",
    "2. Converting all text to lower case - As with the previous point, this help in standardization. Makes the text uniform and minimizes the influence of grammatical errors or stylized text.\n",
    "\n",
    "3. Stopword removal - Stopwords are lexical units that have no semantic meaning; they are present to satisfy the syntactic requirements of the language. Thus, they are of little to no use to us in this assignment. The list of stopwords used in this program was provided on [this website](https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt)\n",
    "\n",
    "4. Training dataset 70/30 split - Since our test dataset does not contain the target values we aim to predict, we must use a subset of our training set for the purposes of fine-tuning the model. Another option to consider would be running a k-fold structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Our Deeds are the Reason of this #earthquake M...\n",
      "1               Forest fire near La Ronge Sask. Canada\n",
      "2    All residents asked to 'shelter in place' are ...\n",
      "3    13,000 people receive #wildfires evacuation or...\n",
      "4    Just got sent this photo from Ruby #Alaska as ...\n",
      "Name: text, dtype: object\n",
      "0    Our Deeds are the Reason of this earthquake Ma...\n",
      "1                Forest fire near La Ronge Sask Canada\n",
      "2    All residents asked to shelter in place are be...\n",
      "3    13000 people receive wildfires evacuation orde...\n",
      "4    Just got sent this photo from Ruby Alaska as s...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation to allow for better word manipulation, except in hyphenation cases, an exception we're willing to make given that this is not a deep linguistic analisys. However, if we wanted to maintain the most semantic context, a name entity recognition system should parse this text beforehand.\n",
    "import string\n",
    "\n",
    "print(og_train_df['text'].head())\n",
    "\n",
    "# Remove punctuation using translate\n",
    "og_train_df[\"text\"] = og_train_df[\"text\"].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "print(og_train_df['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    our deeds are the reason of this earthquake ma...\n",
      "1                forest fire near la ronge sask canada\n",
      "2    all residents asked to shelter in place are be...\n",
      "3    13000 people receive wildfires evacuation orde...\n",
      "4    just got sent this photo from ruby alaska as s...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert text to lowercase\n",
    "og_train_df['text'] = og_train_df['text'].str.lower()\n",
    "print(og_train_df[\"text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                deeds reason earthquake allah forgive\n",
      "1                             forest ronge sask canada\n",
      "2    residents asked shelter place notified officer...\n",
      "3    13000 people receive wildfires evacuation orde...\n",
      "4       photo ruby alaska smoke wildfires pours school\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get the list of stopwords to be removed (stopwords list not mine)\n",
    "import requests\n",
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = set(stopwords_list.decode().splitlines()) \n",
    "\n",
    "# This function takes a string and checks if each of its words is in the stopword list. If so, it removes it from the string\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  # Ensure the value is a string\n",
    "        return \" \".join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "og_train_df['text'] = og_train_df['text'].apply(remove_stopwords)\n",
    "\n",
    "print(og_train_df[\"text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split DataFrame into 70% train and 30% test\n",
    "train_df, test_df = train_test_split(og_train_df, test_size=0.3, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
